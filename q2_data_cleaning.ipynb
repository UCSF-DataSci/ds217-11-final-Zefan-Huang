{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b3d469",
   "metadata": {},
   "source": [
    "# Q2: Data Cleaning\n",
    "\n",
    "**Phase 3:** Data Cleaning & Preprocessing  \n",
    "**Points: 9 points**\n",
    "\n",
    "**Focus:** Handle missing data, outliers, validate data types, remove duplicates.\n",
    "\n",
    "**Lecture Reference:** Lecture 11, Notebook 1 ([`11/demo/01_setup_exploration_cleaning.ipynb`](https://github.com/christopherseaman/datasci_217/blob/main/11/demo/01_setup_exploration_cleaning.ipynb)), Phase 3. Also see Lecture 05 (data cleaning).\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "a377e3f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T06:37:27.928719Z",
     "start_time": "2025-12-08T06:37:27.748399Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load data from Q1 (or directly from source)\n",
    "df = pd.read_csv('data/beach_sensors.csv')\n",
    "# If you saved cleaned data from Q1, you can load it:\n",
    "# df = pd.read_csv('output/q1_exploration.csv')  # This won't work - load original"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "86e6b0ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Clean the dataset by handling missing data, outliers, validating data types, and removing duplicates.\n",
    "\n",
    "**Time Series Note:** For time series data, forward-fill (`ffill()`) is often appropriate for missing values since sensor readings are continuous. However, you may choose other strategies based on your analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Required Artifacts\n",
    "\n",
    "You must create exactly these 3 files in the `output/` directory:\n",
    "\n",
    "### 1. `output/q2_cleaned_data.csv`\n",
    "**Format:** CSV file\n",
    "**Content:** Cleaned dataset with same structure as original (same columns)\n",
    "**Requirements:**\n",
    "- Same columns as original dataset\n",
    "- Missing values handled (filled, dropped, or imputed)\n",
    "- Outliers handled (removed, capped, or transformed)\n",
    "- Data types validated and converted\n",
    "- Duplicates removed\n",
    "- **Sanity check:** Dataset should retain most rows after cleaning (at least 1,000 rows). If you're removing more than 50% of data, reconsider your strategy—imputation is usually preferable to dropping rows for this dataset.\n",
    "- **No index column** (save with `index=False`)\n",
    "\n",
    "### 2. `output/q2_cleaning_report.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Detailed report of cleaning operations\n",
    "**Required information:**\n",
    "- Rows before cleaning: [number]\n",
    "- Missing data handling method: [description]\n",
    "  - Which columns had missing data\n",
    "  - Method used (drop, forward-fill, impute, etc.)\n",
    "  - Number of values handled\n",
    "- Outlier handling: [description]\n",
    "  - Detection method (IQR, z-scores, domain knowledge)\n",
    "  - Which columns had outliers\n",
    "  - Method used (remove, cap, transform)\n",
    "  - Number of outliers handled\n",
    "- Duplicates removed: [number]\n",
    "- Data type conversions: [list any conversions]\n",
    "- Rows after cleaning: [number]\n",
    "\n",
    "**Example format:**\n",
    "```\n",
    "DATA CLEANING REPORT\n",
    "====================\n",
    "\n",
    "Rows before cleaning: 50000\n",
    "\n",
    "Missing Data Handling:\n",
    "- Water Temperature: 2500 missing values (5.0%)\n",
    "  Method: Forward-fill (time series appropriate)\n",
    "  Result: All missing values filled\n",
    "  \n",
    "- Air Temperature: 1500 missing values (3.0%)\n",
    "  Method: Forward-fill, then median imputation for remaining\n",
    "  Result: All missing values filled\n",
    "\n",
    "Outlier Handling:\n",
    "- Water Temperature: Detected 500 outliers using IQR method (3×IQR)\n",
    "  Method: Capped at bounds [Q1 - 3×IQR, Q3 + 3×IQR]\n",
    "  Bounds: [-5.2, 35.8]\n",
    "  Result: 500 values capped\n",
    "\n",
    "Duplicates Removed: 0\n",
    "\n",
    "Data Type Conversions:\n",
    "- Measurement Timestamp: Converted to datetime64[ns]\n",
    "\n",
    "Rows after cleaning: 50000\n",
    "```\n",
    "\n",
    "### 3. `output/q2_rows_cleaned.txt`\n",
    "**Format:** Plain text file\n",
    "**Content:** Single integer number (total rows after cleaning)\n",
    "**Requirements:**\n",
    "- Only the number, no text, no labels\n",
    "- No whitespace before or after\n",
    "- Example: `50000`\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements Checklist\n",
    "\n",
    "- [ ] Missing data handling strategy chosen and implemented\n",
    "- [ ] Outliers detected and handled (IQR method, z-scores, or domain knowledge)\n",
    "- [ ] Data types validated and converted\n",
    "- [ ] Duplicates identified and removed\n",
    "- [ ] Cleaning decisions documented in report\n",
    "- [ ] All 3 required artifacts saved with exact filenames\n",
    "\n",
    "---\n",
    "\n",
    "## Your Approach\n",
    "\n",
    "1. **Handle missing data** - Choose appropriate strategy (drop, forward-fill, impute) based on data characteristics\n",
    "2. **Detect and handle outliers** - Use IQR method or z-scores; decide whether to remove, cap, or transform\n",
    "3. **Validate data types** - Ensure numeric and datetime columns are properly typed\n",
    "4. **Remove duplicates**\n",
    "5. **Document and save** - Write detailed cleaning report explaining your decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Points\n",
    "\n",
    "- **Missing data:** Should you drop rows, impute values, or forward-fill? Consider: How much data is missing? Is it random or systematic? For time series, forward-fill is often appropriate.\n",
    "- **Outliers:** Are they errors or valid extreme values? Use IQR method or z-scores to detect, then decide: remove, cap, or transform. Document your reasoning.\n",
    "- **Data types:** Are numeric columns actually numeric? Are datetime columns properly formatted? Convert as needed.\n",
    "\n",
    "---\n",
    "\n",
    "## Checkpoint\n",
    "\n",
    "After Q2, you should have:\n",
    "- [ ] Missing data handled\n",
    "- [ ] Outliers addressed\n",
    "- [ ] Data types validated\n",
    "- [ ] Duplicates removed\n",
    "- [ ] All 3 artifacts saved: `q2_cleaned_data.csv`, `q2_cleaning_report.txt`, `q2_rows_cleaned.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Continue to `q3_data_wrangling.md` for Data Wrangling.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 1:",
   "id": "854851ad3def6830"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T08:16:44.304930Z",
     "start_time": "2025-12-08T08:16:43.114139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clean_df = pd.read_csv('data/beach_sensors.csv')\n",
    "rows_before = clean_df.shape[0]\n",
    "\n",
    "clean_df[\"Measurement Timestamp\"] = pd.to_datetime(clean_df[\"Measurement Timestamp\"], errors=\"coerce\")\n",
    "numeric_cols = clean_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = clean_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "missing_before = clean_df.isna().sum()\n",
    "\n",
    "##Missing value:\n",
    "for col in numeric_cols:\n",
    "    clean_df[col] = clean_df[col].fillna(clean_df[col].median())\n",
    "for col in categorical_cols:\n",
    "    clean_df[col] = clean_df[col].fillna(\"Unknown\")\n",
    "\n",
    "# record outliers before cleaning\n",
    "outliers_before = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = clean_df[col].quantile(0.25)\n",
    "    Q3 = clean_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    count = ((clean_df[col] < lower) | (clean_df[col] > upper)).sum()\n",
    "    outliers_before[col] = int(count)\n",
    "\n",
    "##outliers handling\n",
    "for col in numeric_cols:\n",
    "    Q1 = clean_df[col].quantile(0.25)\n",
    "    Q3 = clean_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    clean_df[col] = clean_df[col].clip(lower, upper)\n",
    "\n",
    "##no duplicates, so part 2 I can just do the text report\n",
    "rows_before_dedup = clean_df.shape[0]\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "duplicates_removed = rows_before_dedup - clean_df.shape[0]\n",
    "\n",
    "rows_after = clean_df.shape[0]\n",
    "\n",
    "##save\n",
    "clean_df.to_csv(\"output/q2_cleaned_data.csv\", index=False)\n",
    "print(\"Saved to output/q2_cleaned_data.csv\")\n",
    "print(\"Final shape:\", clean_df.shape)\n"
   ],
   "id": "414514394c0e89a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to output/q2_cleaned_data.csv\n",
      "Final shape: (196526, 18)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2:",
   "id": "2d556ef1cb641d2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T08:18:09.725863Z",
     "start_time": "2025-12-08T08:18:09.717983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report = \"\"\n",
    "\n",
    "report += \"DATA CLEANING REPORT\\n\"\n",
    "report += \"====================\\n\\n\"\n",
    "\n",
    "report += f\"Rows before cleaning: {rows_before}\\n\\n\"\n",
    "\n",
    "#Missing Data\n",
    "report += \"Missing Data Handling:\\n\"\n",
    "for col in clean_df.columns:\n",
    "    before = int(missing_before[col])\n",
    "    if before > 0:\n",
    "        if col in numeric_cols:\n",
    "            method = \"Filled with median\"\n",
    "        else:\n",
    "            method = \"Filled with 'Unknown'\"\n",
    "        report += f\"- {col}: {before} missing values\\n\"\n",
    "        report += f\"  Method: {method}\\n\\n\"\n",
    "\n",
    "if missing_before.sum() == 0:\n",
    "    report += \"- No missing values found.\\n\\n\"\n",
    "\n",
    "#Outliers\n",
    "report += \"Outlier Handling:\\n\"\n",
    "any_outliers = False\n",
    "for col in numeric_cols:\n",
    "    count = outliers_before[col]\n",
    "    if count > 0:\n",
    "        any_outliers = True\n",
    "        report += f\"- {col}: {count} outliers detected (IQR method)\\n\"\n",
    "        report += \"  Method: Capped using IQR bounds\\n\\n\"\n",
    "\n",
    "if not any_outliers:\n",
    "    report += \"- No numeric outliers detected.\\n\\n\"\n",
    "\n",
    "#Duplicate\n",
    "report += f\"Duplicates Removed: {duplicates_removed}\\n\\n\"\n",
    "\n",
    "#Data type\n",
    "report += \"Data Type Conversions:\\n\"\n",
    "report += \"- Measurement Timestamp converted to datetime\\n\\n\"\n",
    "\n",
    "report += f\"Rows after cleaning: {rows_after}\\n\"\n",
    "print(report)\n",
    "\n",
    "#Save\n",
    "with open(\"output/q2_cleaning_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n",
    "print(\"yeah saved! output/q2_cleaning_report.txt\")\n"
   ],
   "id": "2370491e1d18546e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING REPORT\n",
      "====================\n",
      "\n",
      "Rows before cleaning: 196526\n",
      "\n",
      "Missing Data Handling:\n",
      "- Air Temperature: 75 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "- Wet Bulb Temperature: 76054 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "- Rain Intensity: 76054 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "- Total Rain: 76054 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "- Precipitation Type: 76054 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "- Barometric Pressure: 146 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "- Heading: 76054 missing values\n",
      "  Method: Filled with median\n",
      "\n",
      "Outlier Handling:\n",
      "- Air Temperature: 97 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Wet Bulb Temperature: 13308 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Humidity: 185 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Rain Intensity: 4250 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Interval Rain: 15862 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Total Rain: 34745 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Precipitation Type: 8427 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Wind Speed: 12228 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Maximum Wind Speed: 4024 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Barometric Pressure: 4611 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Solar Radiation: 29571 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Heading: 86626 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "- Battery Life: 6 outliers detected (IQR method)\n",
      "  Method: Capped using IQR bounds\n",
      "\n",
      "Duplicates Removed: 0\n",
      "\n",
      "Data Type Conversions:\n",
      "- Measurement Timestamp converted to datetime\n",
      "\n",
      "Rows after cleaning: 196526\n",
      "\n",
      "yeah saved! output/q2_cleaning_report.txt\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3:",
   "id": "3356ce4e2e1e5e13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T08:19:39.355152Z",
     "start_time": "2025-12-08T08:19:39.350261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rows_after = clean_df.shape[0]\n",
    "print(rows_after)\n",
    "\n",
    "with open(\"output/q2_rows_cleaned.txt\", \"w\") as f:\n",
    "    f.write(str(rows_after))\n",
    "\n",
    "print(\"Saved to output/q2_rows_cleaned.txt\")"
   ],
   "id": "781ff44ceb4cf566",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196526\n",
      "Saved to output/q2_rows_cleaned.txt\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
